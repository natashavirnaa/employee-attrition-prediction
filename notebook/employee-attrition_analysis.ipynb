{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7f2e376",
      "metadata": {
        "id": "d7f2e376"
      },
      "source": [
        "# **PROJECT STATISTICAL MACHINE LEARNING**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c443045a",
      "metadata": {
        "id": "c443045a"
      },
      "source": [
        "## **📝 Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "d4d8ff93",
      "metadata": {
        "id": "d4d8ff93"
      },
      "outputs": [],
      "source": [
        "#Import Library\n",
        "import argparse\n",
        "import warnings\n",
        "from typing import Dict , Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "from tabulate import tabulate\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, make_scorer, roc_curve, auc\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    ExtraTreesClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    BaggingClassifier,\n",
        "    HistGradientBoostingClassifier\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV  #Import CalibratedClassifierCV\n",
        "from scipy.stats import randint, uniform\n",
        "from itertools import product\n",
        "\n",
        "#Untuk menonaktifkan pesan peringatan yang biasanya muncul\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RANDOM_STATE = 42\n",
        "TARGET = \"Attrition\"\n",
        "ID_COL = \"id\"\n",
        "\n",
        "# ===== Library Advanced =====\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except Exception:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except Exception:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    CATBOOST_AVAILABLE = True\n",
        "except Exception:\n",
        "    CATBOOST_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7aefd4",
      "metadata": {
        "id": "2e7aefd4"
      },
      "source": [
        "## **⚙️1. Data Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "a5a57dee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5a57dee",
        "outputId": "1cc2b6fa-f5b2-41c8-fb83-6fb99b4fc6b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 1: DATA LOADING\n",
            "--------------------------------------------------------------------------------\n",
            "Train: (1176, 36), Test: (294, 35)\n",
            "\n",
            "Class 0: 986 (83.8%)\n",
            "Class 1: 190 (16.2%)\n",
            "Ratio: 5.2:1\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 1: DATA LOADING\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Load Dataset - Update the file paths to use the correct location in the Colab environment\n",
        "train = pd.read_csv('C:/ITS/MATKUL/SEMESTER 5/SML/PRAKTIKUM SML/PROJECT SML/train.csv')\n",
        "test = pd.read_csv('C:/ITS/MATKUL/SEMESTER 5/SML/PRAKTIKUM SML/PROJECT SML/test.csv')\n",
        "\n",
        "print(f\"Train: {train.shape}, Test: {test.shape}\")\n",
        "\n",
        "#Check Class Distribution\n",
        "target_counts = train['Attrition'].value_counts()\n",
        "\n",
        "print(f\"\\nClass 0: {target_counts[0]} ({target_counts[0]/len(train)*100:.1f}%)\")\n",
        "print(f\"Class 1: {target_counts[1]} ({target_counts[1]/len(train)*100:.1f}%)\")\n",
        "print(f\"Ratio: {target_counts[0]/target_counts[1]:.1f}:1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b5625c7",
      "metadata": {
        "id": "2b5625c7"
      },
      "source": [
        "## **🛠️ Pre-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "df28ec8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df28ec8f",
        "outputId": "56aa630e-2c3e-46f2-b094-bfae798c4fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 2: PREPROCESSING\n",
            "--------------------------------------------------------------------------------\n",
            "Removed: ['EmployeeCount', 'Over18', 'StandardHours']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 2: PREPROCESSING\")\n",
        "print(\"-\"*80)\n",
        "#This section handles data cleaning and encoding before model training\n",
        "\n",
        "# ---- Separate features and target variable ----\n",
        "# Drop 'Attrition' and 'id' from training data, keep 'Attrition' as target\n",
        "X = train.drop(['Attrition', 'id'], axis=1)\n",
        "y = train['Attrition']\n",
        "\n",
        "#Drop 'id' column from test data and store the IDs separately\n",
        "X_test = test.drop(['id'], axis=1)\n",
        "test_ids = test['id'].copy()\n",
        "\n",
        "# ---- Remove constant columns ----\n",
        "# Identify columns with only unique value (no variation) \n",
        "constant_cols = [col for col in X.columns if X[col].nunique() == 1]\n",
        "\n",
        "if constant_cols:\n",
        "    X = X.drop(constant_cols, axis=1)\n",
        "    X_test = X_test.drop(constant_cols, axis=1)\n",
        "    print(f\"Removed: {constant_cols}\")\n",
        "\n",
        "# ---- Encode categorical variables ----\n",
        "# Convert categorical (object-type) columns into numeric labels \n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    X_test[col] = le.transform(X_test[col].astype(str))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8365663d",
      "metadata": {
        "id": "8365663d"
      },
      "source": [
        "## **⚓3. Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "ee9cc808",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9cc808",
        "outputId": "644348c0-b23f-4e1d-ce88-28e256383f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 3: FEATURE ENGINEERING\n",
            "--------------------------------------------------------------------------------\n",
            "Features: 31 -> 51\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 3: FEATURE ENGINEERING\")\n",
        "print(\"-\"*80)\n",
        "#This section creates new engineered features to better capture\n",
        "\n",
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1) Core Ratio Features\n",
        "    df['ExperienceRatio'] = df['YearsAtCompany'] / (df['TotalWorkingYears'] + 1)\n",
        "    df['CurrentRoleRatio'] = df['YearsInCurrentRole'] / (df['YearsAtCompany'] + 1)\n",
        "    df['JobHoppingRate'] = df['NumCompaniesWorked'] / (df['TotalWorkingYears'] + 1)\n",
        "    df['ManagerStability'] = df['YearsWithCurrManager'] / (df['YearsAtCompany'] + 1)\n",
        "\n",
        "    # 2) Binary Indicators\n",
        "    df['IsYoung'] = (df['Age'] < 30).astype(int)\n",
        "    df['TimeWithoutPromotion'] = (df['YearsSinceLastPromotion'] > 3).astype(int)\n",
        "    df['LowJobLevel'] = (df['JobLevel'] <= 1).astype(int)\n",
        "    df['LongCommute'] = (df['DistanceFromHome'] > 15).astype(int)\n",
        "    df['PoorWorkLife'] = (df['WorkLifeBalance'] <= 2).astype(int)\n",
        "    df['OverTime_Binary'] = df['OverTime']\n",
        "\n",
        "    # 3) Satisfaction & Career Dynamics\n",
        "    satisfaction_cols = [\n",
        "        'EnvironmentSatisfaction', 'JobSatisfaction',\n",
        "        'RelationshipSatisfaction', 'WorkLifeBalance'\n",
        "    ]\n",
        "    df['AvgSatisfaction'] = df[satisfaction_cols].mean(axis=1)\n",
        "    df['LowSatisfaction'] = (df['AvgSatisfaction'] < 2).astype(int)\n",
        "    df['Career_Stagnation'] = (\n",
        "        (df['YearsSinceLastPromotion'] > 5) & (df['YearsAtCompany'] > 10)\n",
        "    ).astype(int)\n",
        "\n",
        "    # 4) Combined Risk Score\n",
        "    df['AttritionRiskScore'] = (\n",
        "        df['LowSatisfaction'] * 2.5 +\n",
        "        df['OverTime_Binary'] * 2.0 +\n",
        "        df['LongCommute'] * 1.0 +\n",
        "        df['TimeWithoutPromotion'] * 2.0 +\n",
        "        df['PoorWorkLife'] * 1.5 +\n",
        "        df['LowJobLevel'] * 0.5 +\n",
        "        df['Career_Stagnation'] * 2.0 +\n",
        "        df['JobHoppingRate'] * 10\n",
        "    )\n",
        "\n",
        "    # 5) Extended Features (optional)\n",
        "    df['IncomePerYear'] = df['MonthlyIncome'] / (df['TotalWorkingYears'] + 1)\n",
        "    df['Income_JobLevel'] = df['MonthlyIncome'] * df['JobLevel']\n",
        "    df['Age_Experience'] = df['Age'] * df['TotalWorkingYears']\n",
        "    df['Satisfaction_Performance'] = df['AvgSatisfaction'] * df['PerformanceRating']\n",
        "    df['Seniority'] = df['JobLevel'] * df['TotalWorkingYears']\n",
        "    df['Promotion_Rate'] = df['YearsAtCompany'] / (df['YearsSinceLastPromotion'] + 1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply feature creation to train and test sets\n",
        "X_eng = create_features(X)\n",
        "X_test_eng = create_features(X_test)\n",
        "\n",
        "print(f\"Features: {X.shape[1]} -> {X_eng.shape[1]}\"); "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d390815",
      "metadata": {
        "id": "9d390815"
      },
      "source": [
        "## **🔍4. Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "2735cac7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2735cac7",
        "outputId": "55b6d9dc-6499-4d4c-fd61-a37613cccef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 4: FEATURE SELECTION\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " ===== Top 20 Fitur Terpenting =====\n",
            " 1. AttritionRiskScore                    0.077714\n",
            " 2. Age_Experience                        0.059433\n",
            " 3. Income_JobLevel                       0.044588\n",
            " 4. Age                                   0.042031\n",
            " 5. MonthlyIncome                         0.041789\n",
            " 6. JobHoppingRate                        0.040061\n",
            " 7. Seniority                             0.032116\n",
            " 8. IncomePerYear                         0.030065\n",
            " 9. TotalWorkingYears                     0.028681\n",
            "10. HourlyRate                            0.028677\n",
            "11. DailyRate                             0.028454\n",
            "12. DistanceFromHome                      0.028162\n",
            "13. AvgSatisfaction                       0.027116\n",
            "14. EmployeeNumber                        0.025803\n",
            "15. MonthlyRate                           0.025095\n",
            "16. OverTime_Binary                       0.024646\n",
            "17. Satisfaction_Performance              0.023306\n",
            "18. ManagerStability                      0.022966\n",
            "19. OverTime                              0.022844\n",
            "20. StockOptionLevel                      0.022248\n",
            "\n",
            "=====Selected Top 51 features:=====\n",
            "['AttritionRiskScore', 'Age_Experience', 'Income_JobLevel', 'Age', 'MonthlyIncome', 'JobHoppingRate', 'Seniority', 'IncomePerYear', 'TotalWorkingYears', 'HourlyRate'] ...\n",
            "\n",
            "Final shape -> X_sel: (1176, 51), X_test_sel: (294, 51)\n",
            "\n",
            "Feature importance chart saved as 'feature_importance.png' ✅\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 4: FEATURE SELECTION\")\n",
        "print(\"-\"*80)\n",
        "#This section uses a Random Forest model to evaluate feature importance and save the visualization as an image file instead of displaying it.\n",
        "\n",
        "# 1) Train Random Forest model to get feature importances\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_eng, y)\n",
        "\n",
        "# 2️) Create a DataFrame of features and their importances\n",
        "feat_imp = (\n",
        "    pd.DataFrame({\n",
        "    'feature': X_eng.columns,\n",
        "    'importance': rf.feature_importances_\n",
        "    })\n",
        "    .sort_values('importance', ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 3️) Print Top 20 Most Important Features\n",
        "print(\"\\n ===== Top 20 Fitur Terpenting =====\")\n",
        "for i, (f, imp) in enumerate(zip(feat_imp['feature'][:20], feat_imp['importance'][:20]), 1):\n",
        "    print(f\"{i:2d}. {f.ljust(35)} {imp:>10.6f}\")\n",
        "\n",
        "# 4️) Select top features (Max 60 Variable)\n",
        "N_FEAT = min(60, len(feat_imp))\n",
        "selected = feat_imp.head(N_FEAT)['feature'].tolist()\n",
        "\n",
        "# Save Datasets with Selected Features\n",
        "X_sel = X_eng[selected].copy()\n",
        "X_test_sel = X_test_eng[selected].copy()\n",
        "\n",
        "print(f\"\\n=====Selected Top {N_FEAT} features:=====\")\n",
        "print(selected[:10], \"...\" if len(selected) > 10 else \"\")\n",
        "print(f\"\\nFinal shape -> X_sel: {X_sel.shape}, X_test_sel: {X_test_sel.shape}\")\n",
        "\n",
        "# 5️) Visualize and save feature importance chart (no display)\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "top = feat_imp.head(30)\n",
        "ax.barh(top['feature'], top['importance'], color='lightpink')\n",
        "ax.set_xlabel('Importance', fontsize=11)\n",
        "ax.set_ylabel('Feature', fontsize=11)\n",
        "ax.set_title('Feature Importances', fontsize=13, fontweight='bold')\n",
        "ax.invert_yaxis()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save to file\n",
        "output_path = \"feature_importance.png\"\n",
        "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"\\nFeature importance chart saved as '{output_path}' ✅\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "776635cb",
      "metadata": {
        "id": "776635cb"
      },
      "source": [
        "## **🕹️5. Split Data Training and Data Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "35afd5dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35afd5dc",
        "outputId": "97bb8b2b-b845-4681-9d7a-266f07a705f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 5: TRAIN–VAL SPLIT\n",
            "--------------------------------------------------------------------------------\n",
            "Train: (940, 51), Val: (236, 51)\n",
            "\n",
            "===== Distribusi Target (setelah stratify): =====\n",
            "Train set:\n",
            "Attrition\n",
            "0   0.8380\n",
            "1   0.1620\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Validation set:\n",
            "Attrition\n",
            "0   0.8390\n",
            "1   0.1610\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Data scaling completed successfully ✅\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 5: TRAIN–VAL SPLIT\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# 1️) S plit data (80:20) with stratification\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_sel, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "\n",
        "# 2️) Check target distribution in train and validation sets\n",
        "print(\"\\n===== Distribusi Target (setelah stratify): =====\")\n",
        "train_dist = y_train.value_counts(normalize=True).round(3)\n",
        "val_dist = y_val.value_counts(normalize=True).round(3)\n",
        "\n",
        "print(\"Train set:\")\n",
        "print(train_dist)\n",
        "print(\"\\nValidation set:\")\n",
        "print(val_dist)\n",
        "\n",
        "# 3️) Feature scaling using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_sc = scaler.fit_transform(X_train)\n",
        "X_val_sc = scaler.transform(X_val)\n",
        "\n",
        "print(\"\\nData scaling completed successfully ✅\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0e2bead",
      "metadata": {
        "id": "b0e2bead"
      },
      "source": [
        "## **🏛️6. Class Balancing and HyperParameter Tunning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "6b25b430",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "6b25b430",
        "outputId": "74eb385a-19cf-42d8-d179-dc3e2aac19e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 6 : CLASS BALANCING AND HYPERPARAMETER TUNNING\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            " SECTION 6A : CLASS BLANCING\n",
            "--------------------------------------------------------------------------------\n",
            "Not using SMOTE — original data will be trained with class_weight balancing\n",
            "\n",
            "SECTION 6B: HYPERPARAMETER TUNING (Fast + Smart)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "♦ Tuning LightGBM Hyperparameters (Fast Mode)...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "\n",
            "✅ Best LightGBM Params:\n",
            "{'colsample_bytree': np.float64(0.9976634677873653), 'learning_rate': np.float64(0.028524445288831496), 'max_depth': 4, 'min_child_samples': 46, 'n_estimators': 652, 'num_leaves': 36, 'reg_alpha': np.float64(0.06918727512424727), 'reg_lambda': np.float64(4.148647961550335), 'subsample': np.float64(0.8199582915145766)}\n",
            "Best CV AUC: 0.8220\n",
            "\n",
            "♦ Tuning XGBoost Hyperparameters (Fast Mode)...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "\n",
            "✅ Best XGBoost Params:\n",
            "{'colsample_bytree': np.float64(0.9181815987569262), 'gamma': np.float64(0.6530815376116708), 'learning_rate': np.float64(0.03852219872026997), 'max_depth': 3, 'min_child_weight': 7, 'n_estimators': 695, 'reg_alpha': np.float64(2.533601546034454), 'reg_lambda': np.float64(6.978560881099047), 'subsample': np.float64(0.8619076397167239)}\n",
            "Best CV AUC: 0.8217\n",
            "\n",
            "🎯 DONE: Class balancing and hyperparameter tuning completed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 6 : CLASS BALANCING AND HYPERPARAMETER TUNNING\")\n",
        "print(\"-\" * 80)\n",
        "#This section handles dataset balancing using SMOTE (Optional) and performs hyperprameter tuning for LightGBM and XBoost Models\n",
        "\n",
        "print(\"\\n SECTION 6A : CLASS BLANCING\")\n",
        "print(\"-\" * 80) \n",
        "\n",
        "# You can toggle this flag if you want to try SMOTE\n",
        "USE_SMOTE = False  # default : not use SMOTE\n",
        "IMBLEARN_AVAILABLE = True # ENsure imbalanced-learn is available\n",
        "\n",
        "if IMBLEARN_AVAILABLE and USE_SMOTE:\n",
        "    # 1) Combine SMOTE and RandomUnderSampler\n",
        "    smote = SMOTE(sampling_strategy=0.3, random_state=42, k_neighbors=3)\n",
        "    under = RandomUnderSampler(sampling_strategy=0.8, random_state=42)\n",
        "\n",
        "    # 2) Oversampling followed by undersampling\n",
        "    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
        "    X_train_bal, y_train_bal = under.fit_resample(X_train_bal, y_train_bal)\n",
        "\n",
        "    # 3) Re-scale data since SMOTE introduces new samples\n",
        "    X_train_bal_sc = scaler.fit_transform(X_train_bal)\n",
        "\n",
        "    print(f\"Balanced: {X_train.shape[0]} → {X_train_bal.shape[0]} samples\")\n",
        "    print(\"Distribusi label setelah balancing:\")\n",
        "    print(pd.Series(y_train_bal).value_counts(normalize=True).round(3))\n",
        "\n",
        "    # 4) Visualize class distribution before and after balancing\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    colors = [\"skyblue\", \"hotpink\"]\n",
        "\n",
        "    # Before balancing\n",
        "    axes[0].bar(['No', 'Yes'], y_train.value_counts().sort_index(), color=colors)\n",
        "    axes[0].set_title('Original', fontweight='bold')\n",
        "    axes[0].set_ylabel('Count')\n",
        "\n",
        "    # After balancing\n",
        "    axes[1].bar(['No', 'Yes'], pd.Series(y_train_bal).value_counts().sort_index(), color=colors)\n",
        "    axes[1].set_title('After SMOTE + UnderSampling', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('balancing.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "else:\n",
        "    # Without SMOTE — use original data with class_weight in model training\n",
        "    X_train_bal, y_train_bal = X_train.copy(), y_train.copy()\n",
        "    X_train_bal_sc = X_train_sc.copy()\n",
        "    print(\"Not using SMOTE — original data will be trained with class_weight balancing\")\n",
        "\n",
        "\n",
        "print(\"\\nSECTION 6B: HYPERPARAMETER TUNING (Fast + Smart)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "#Setup Stratified K-Fold Cross Validation\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# ===== Tuning LightGBM (FAST) =====\n",
        "print(\"\\n♦ Tuning LightGBM Hyperparameters (Fast Mode)...\")\n",
        "\n",
        "param_dist_lgb = {\n",
        "    'num_leaves': randint(25, 45), 'learning_rate': uniform(0.01, 0.03),\n",
        "    'n_estimators': randint(400, 900), 'min_child_samples': randint(25, 55),\n",
        "    'subsample': uniform(0.7, 0.3), 'colsample_bytree': uniform(0.7, 0.3),\n",
        "    'reg_lambda': uniform(1, 6), 'reg_alpha': uniform(0, 3),\n",
        "    'max_depth': [-1, 4, 5, 6]\n",
        "}\n",
        "\n",
        "lgb_base = LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)\n",
        "search_lgb = RandomizedSearchCV(\n",
        "    estimator=lgb_base, param_distributions=param_dist_lgb,\n",
        "    n_iter=10, scoring='roc_auc', cv=cv,\n",
        "    random_state=42, n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "search_lgb.fit(X_train_bal, y_train_bal)\n",
        "print(f\"\\n✅ Best LightGBM Params:\\n{search_lgb.best_params_}\")\n",
        "print(f\"Best CV AUC: {search_lgb.best_score_:.4f}\")\n",
        "best_lgb_params = search_lgb.best_params_\n",
        "\n",
        "# ===== Tuning XGBoost (FAST) =====\n",
        "try:\n",
        "    print(\"\\n♦ Tuning XGBoost Hyperparameters (Fast Mode)...\")\n",
        "\n",
        "    param_dist_xgb = {\n",
        "        'max_depth': randint(3, 6), 'learning_rate': uniform(0.01, 0.05),\n",
        "        'n_estimators': randint(400, 800), 'min_child_weight': randint(2, 8),\n",
        "        'subsample': uniform(0.7, 0.3), 'colsample_bytree': uniform(0.7, 0.3),\n",
        "        'gamma': uniform(0, 2), 'reg_alpha': uniform(0, 3), 'reg_lambda': uniform(1, 8)\n",
        "    }\n",
        "\n",
        "    xgb_base = XGBClassifier(\n",
        "        eval_metric='auc', random_state=42, n_jobs=-1, use_label_encoder=False,\n",
        "    )\n",
        "\n",
        "    search_xgb = RandomizedSearchCV(\n",
        "        estimator=xgb_base, param_distributions=param_dist_xgb,\n",
        "        n_iter=10, scoring='roc_auc', cv=cv,\n",
        "        random_state=42, n_jobs=-1, verbose=1\n",
        "    )\n",
        "    search_xgb.fit(X_train_bal, y_train_bal)\n",
        "    print(f\"\\n✅ Best XGBoost Params:\\n{search_xgb.best_params_}\")\n",
        "    print(f\"Best CV AUC: {search_xgb.best_score_:.4f}\")\n",
        "    best_xgb_params = search_xgb.best_params_\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n Skipping XGBoost tuning (error or not installed): {e}\")\n",
        "\n",
        "print(\"\\n🎯 DONE: Class balancing and hyperparameter tuning completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d062771f",
      "metadata": {
        "id": "d062771f"
      },
      "source": [
        "## **🏃🏻‍♀️7. MODEL TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "kmnivAYGEZdB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmnivAYGEZdB",
        "outputId": "f76bc0c1-41d5-4234-a224-5f3814280e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 7: MODEL TRAINING (TUNED & Optimized)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====== LINEAR MODELS (Regularized) ======\n",
            "\n",
            "Training Logistic L2 (Tuned)...\n",
            " Train: 0.8496, Val: 0.8321, CV: 0.8144, Gap: 0.0175 (GOOD)\n",
            "\n",
            "Training Logistic L1 (Tuned)...\n",
            " Train: 0.6921, Val: 0.7275, CV: 0.6387, Gap: -0.0354 (GOOD)\n",
            "\n",
            "===== TREE MODELS (Tuned for Generalization) =====\n",
            "\n",
            "Training Random Forest (Optimized)...\n",
            " Train: 0.9187, Val: 0.7919, CV: 0.7858, Gap: 0.1269 (BAD)\n",
            "\n",
            "Training Extra Trees (Optimized)...\n",
            " Train: 0.9472, Val: 0.8215, CV: 0.8005, Gap: 0.1256 (BAD)\n",
            "\n",
            "Training Gradient Boosting (Optimized)...\n",
            " Train: 0.9905, Val: 0.8008, CV: 0.8238, Gap: 0.1898 (BAD)\n",
            "\n",
            "Training XGBoost (Optimized)...\n",
            " Train: 0.8963, Val: 0.8091, CV: 0.8156, Gap: 0.0871 (WARN)\n",
            "\n",
            "Training LightGBM (Tuned)...\n",
            " Train: 0.9958, Val: 0.7872, CV: 0.8169, Gap: 0.2086 (BAD)\n",
            "\n",
            "Training CatBoost (Optimized)...\n",
            " Train: 0.9670, Val: 0.8121, CV: 0.8225, Gap: 0.1550 (BAD)\n",
            "\n",
            "====== EXPERIMENTAL MODELS (New) ======\n",
            "\n",
            "Training AdaBoost (Tuned)...\n",
            " Train: 0.8562, Val: 0.7834, CV: 0.7925, Gap: 0.0729 (WARN)\n",
            "\n",
            "Training HistGradientBoosting (Fast)...\n",
            " Train: 1.0000, Val: 0.7648, CV: 0.7991, Gap: 0.2352 (BAD)\n",
            "\n",
            "Training Bagging (Logistic Base)...\n",
            " Train: 0.8365, Val: 0.8345, CV: 0.8110, Gap: 0.0020 (GOOD)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 7: MODEL TRAINING (TUNED & Optimized)\")\n",
        "print(\"-\" * 80)\n",
        "# This section trains multiple ML models using optimized parameters and evaluates performance consistency through ROC-AUC metrics\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 10-Fold CV for more stable evaluation\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def train_model(name, model, use_scaled=False):\n",
        "\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    X_tr = X_train_bal_sc if use_scaled else X_train_bal\n",
        "    y_tr = y_train_bal\n",
        "    X_v = X_val_sc if use_scaled else X_val\n",
        "\n",
        "    model.fit(X_tr, y_tr)\n",
        "    y_tr_pred = model.predict_proba(X_tr)[:, 1]\n",
        "    y_v_pred = model.predict_proba(X_v)[:, 1]\n",
        "\n",
        "    tr_score = roc_auc_score(y_tr, y_tr_pred)\n",
        "    v_score = roc_auc_score(y_val, y_v_pred)\n",
        "\n",
        "    try:\n",
        "        cv_scores = cross_val_score(model, X_tr, y_tr, cv=cv, scoring=\"roc_auc\", n_jobs=-1)\n",
        "        cv_mean, cv_std = cv_scores.mean(), cv_scores.std()\n",
        "    except Exception as e:\n",
        "        print(f\"CV error for {name}: {e}\")\n",
        "        cv_mean, cv_std = v_score, 0.0\n",
        "\n",
        "    gap = tr_score - v_score\n",
        "    status = \"GOOD\" if gap < 0.05 else \"WARN\" if gap < 0.10 else \"BAD\"\n",
        "\n",
        "    print(f\" Train: {tr_score:.4f}, Val: {v_score:.4f}, CV: {cv_mean:.4f}, Gap: {gap:.4f} ({status})\")\n",
        "\n",
        "    results[name] = {\n",
        "        \"model\": model,\n",
        "        \"train\": tr_score,\n",
        "        \"val\": v_score,\n",
        "        \"cv\": cv_mean,\n",
        "        \"cv_std\": cv_std,\n",
        "        \"gap\": gap,\n",
        "        \"pred\": y_v_pred,\n",
        "        \"scaled\": use_scaled,\n",
        "    }\n",
        "\n",
        "# ========================= LINEAR MODELS =========================\n",
        "print(\"\\n====== LINEAR MODELS (Regularized) ======\")\n",
        "\n",
        "train_model(\"Logistic L2 (Tuned)\",\n",
        "    LogisticRegression(max_iter=5000, C=0.05, penalty=\"l2\", solver=\"lbfgs\",\n",
        "                       class_weight=\"balanced\", random_state=42), True)\n",
        "\n",
        "train_model(\"Logistic L1 (Tuned)\",\n",
        "    LogisticRegression(max_iter=5000, C=0.02, penalty=\"l1\", solver=\"saga\",\n",
        "                       class_weight=\"balanced\", random_state=42), True)\n",
        "\n",
        "# ========================= TREE MODELS =========================\n",
        "print(\"\\n===== TREE MODELS (Tuned for Generalization) =====\")\n",
        "\n",
        "train_model(\"Random Forest (Optimized)\",\n",
        "    RandomForestClassifier(n_estimators=500, max_depth=6, min_samples_split=40,\n",
        "                           min_samples_leaf=20, max_features=\"sqrt\", max_samples=0.8,\n",
        "                           bootstrap=True, class_weight=\"balanced\",\n",
        "                           random_state=42, n_jobs=-1), False)\n",
        "\n",
        "train_model(\"Extra Trees (Optimized)\",\n",
        "    ExtraTreesClassifier(n_estimators=700, max_depth=7, min_samples_split=25,\n",
        "                         min_samples_leaf=12, max_features=\"sqrt\",\n",
        "                         class_weight=\"balanced\", random_state=42, n_jobs=-1), False)\n",
        "\n",
        "train_model(\"Gradient Boosting (Optimized)\",\n",
        "    GradientBoostingClassifier(n_estimators=1200, learning_rate=0.01,\n",
        "                               max_depth=3, min_samples_split=30, min_samples_leaf=15,\n",
        "                               subsample=0.75, max_features=\"sqrt\",\n",
        "                               random_state=42), False)\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    train_model(\"XGBoost (Optimized)\",\n",
        "        XGBClassifier(n_estimators=1200, learning_rate=0.015, max_depth=4,\n",
        "                      min_child_weight=8, subsample=0.8, colsample_bytree=0.8,\n",
        "                      reg_alpha=5.0, reg_lambda=25.0, gamma=1.0,\n",
        "                      eval_metric=\"auc\", random_state=42, n_jobs=-1), False)\n",
        "\n",
        "if LIGHTGBM_AVAILABLE:\n",
        "    train_model(\"LightGBM (Tuned)\",\n",
        "        LGBMClassifier(n_estimators=1800, learning_rate=best_lgb_params[\"learning_rate\"],\n",
        "                       num_leaves=best_lgb_params[\"num_leaves\"],\n",
        "                       min_child_samples=best_lgb_params[\"min_child_samples\"],\n",
        "                       subsample=best_lgb_params[\"subsample\"],\n",
        "                       colsample_bytree=best_lgb_params[\"colsample_bytree\"],\n",
        "                       reg_alpha=3.0, reg_lambda=6.0,\n",
        "                       random_state=42, verbose=-1), False)\n",
        "    \n",
        "if CATBOOST_AVAILABLE:\n",
        "    train_model(\"CatBoost (Optimized)\",\n",
        "        CatBoostClassifier(iterations=1000, learning_rate=0.01, depth=4,\n",
        "                           l2_leaf_reg=10.0, subsample=0.7,\n",
        "                           random_state=42, verbose=False), False)\n",
        "    \n",
        "# ========================= EXPERIMENTAL MODELS =========================\n",
        "print(\"\\n====== EXPERIMENTAL MODELS (New) ======\")\n",
        "\n",
        "train_model(\"AdaBoost (Tuned)\",\n",
        "    AdaBoostClassifier(n_estimators=500, learning_rate=0.02,\n",
        "                       random_state=42), False)\n",
        "\n",
        "train_model(\"HistGradientBoosting (Fast)\",\n",
        "    HistGradientBoostingClassifier(learning_rate=0.03, l2_regularization=1.0,\n",
        "                                   max_iter=800, max_depth=4, random_state=42), False)\n",
        "\n",
        "train_model(\"Bagging (Logistic Base)\",\n",
        "    BaggingClassifier(estimator=LogisticRegression(max_iter=3000, C=0.05, solver=\"lbfgs\"),\n",
        "                      n_estimators=60, random_state=42, n_jobs=-1), True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SHLrn--5u11N",
      "metadata": {
        "id": "SHLrn--5u11N"
      },
      "source": [
        "## **👓 8.MODEL COMPARISON**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "7Y8mrWCaulAp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y8mrWCaulAp",
        "outputId": "509430be-9dfc-4aee-ab13-dd1049fda438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "SECTION 8: COMPARISON\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Model                          Train  Val    CV     Gap   \n",
            "Gradient Boosting (Optimized) 0.9905 0.8008 0.8238  0.1898\n",
            "         CatBoost (Optimized) 0.9670 0.8121 0.8225  0.1550\n",
            "             LightGBM (Tuned) 0.9958 0.7872 0.8169  0.2086\n",
            "          XGBoost (Optimized) 0.8963 0.8091 0.8156  0.0871\n",
            "          Logistic L2 (Tuned) 0.8496 0.8321 0.8144  0.0175\n",
            "      Bagging (Logistic Base) 0.8365 0.8345 0.8110  0.0020\n",
            "      Extra Trees (Optimized) 0.9472 0.8215 0.8005  0.1256\n",
            "  HistGradientBoosting (Fast) 1.0000 0.7648 0.7991  0.2352\n",
            "             AdaBoost (Tuned) 0.8562 0.7834 0.7925  0.0729\n",
            "    Random Forest (Optimized) 0.9187 0.7919 0.7858  0.1269\n",
            "          Logistic L1 (Tuned) 0.6921 0.7275 0.6387 -0.0354\n",
            "\n",
            "Best CV: 0.8238, Best Val: 0.8345\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\nSECTION 8: COMPARISON\")\n",
        "print(\"-\"*80)\n",
        "#This section summarizes and visualizes all model performances using ROC-AUC metrics for train, vaidation, and CV Scores\n",
        "\n",
        "# --- Convert results dictionary to DataFrame ---\n",
        "df_res = (\n",
        "    pd.DataFrame({\n",
        "        \"Model\": list(results.keys()),\n",
        "        \"Train\": [v[\"train\"] for v in results.values()],\n",
        "        \"Val\": [v[\"val\"] for v in results.values()],\n",
        "        \"CV\": [v[\"cv\"] for v in results.values()],\n",
        "        \"Gap\": [v[\"gap\"] for v in results.values()]\n",
        "    })\n",
        "    .sort_values(\"CV\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# --- Display Summary ---\n",
        "print(\"\\n\" + df_res.to_string(index=False))\n",
        "print(f\"\\nBest CV: {df_res['CV'].max():.4f}, Best Val: {df_res['Val'].max():.4f}\")\n",
        "\n",
        "# --- Create Visualization ---\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "plt.subplots_adjust(hspace=0.4, wspace=0.3)\n",
        "\n",
        "# Plot 1 : Cross Validation AUC Scores\n",
        "ax = axes[0, 0]\n",
        "sorted_df = df_res.sort_values(\"CV\")\n",
        "colors = [\"skyblue\" if x < 0.05 else \"hotpink\" if x < 0.10 else \"purple\" for x in sorted_df[\"Gap\"]]\n",
        "ax.barh(range(len(sorted_df)), sorted_df[\"CV\"], color=colors)\n",
        "ax.set_yticks(range(len(sorted_df)))\n",
        "ax.set_yticklabels(sorted_df[\"Model\"], fontsize=9)\n",
        "ax.set_xlabel(\"CV ROC-AUC\", fontsize=10)\n",
        "ax.set_title(\"Cross-Validation Scores\", fontweight=\"bold\")\n",
        "\n",
        "# Plot 2 : Train vs Validation AUC\n",
        "ax = axes[0, 1]\n",
        "x = np.arange(len(df_res)); w = 0.35\n",
        "ax.bar(x - w/2, df_res[\"Train\"], width=w, label=\"Train\", color=\"pink\")\n",
        "ax.bar(x + w/2, df_res[\"Val\"], width=w, label=\"Validation\", color=\"hotpink\")\n",
        "ax.set_xticks(x); ax.set_xticklabels(df_res[\"Model\"], rotation=45, ha=\"right\", fontsize=8)\n",
        "ax.set_title(\"Train vs Validation ROC-AUC\", fontweight=\"bold\")\n",
        "ax.legend()\n",
        "\n",
        "# Plot 3 : Overfitting Grup\n",
        "ax = axes[1, 0]\n",
        "colors = [\"skyblue\" if x < 0.05 else \"hotpink\" if x < 0.10 else \"purple\" for x in sorted_df[\"Gap\"]]\n",
        "ax.barh(range(len(df_res)), df_res[\"Gap\"], color=colors)\n",
        "ax.set_yticks(range(len(df_res)))\n",
        "ax.set_yticklabels(df_res[\"Model\"], fontsize=9)\n",
        "ax.axvline(x=0.05, color=\"orange\", linestyle=\"--\", label=\"Mild Overfit\")\n",
        "ax.axvline(x=0.10, color=\"red\", linestyle=\"--\", label=\"Strong Overfit\")\n",
        "ax.legend(fontsize=8)\n",
        "ax.set_xlabel(\"Train-Val Gap\")\n",
        "ax.set_title(\"Overfitting Gap Analysis\", fontweight=\"bold\")\n",
        "\n",
        "# Plot 4 : Generalization Plot (Gap vs CV)\n",
        "ax = axes[1, 1]\n",
        "ax.scatter(df_res[\"Gap\"], df_res[\"CV\"], s=100, alpha=0.7, color=\"blue\")\n",
        "for _, row in df_res.iterrows():\n",
        "    ax.annotate(row[\"Model\"], (row[\"Gap\"], row[\"CV\"]), fontsize=8, ha=\"right\")\n",
        "ax.set_xlabel(\"Train–Val Gap\", fontsize=10)\n",
        "ax.set_ylabel(\"CV ROC-AUC\", fontsize=10)\n",
        "ax.set_title(\"Generalization Scatter Plot\", fontweight=\"bold\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "#--- Save Visualization ---\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HzXOQ9_7vdqS",
      "metadata": {
        "id": "HzXOQ9_7vdqS"
      },
      "source": [
        "## **⚖️ 9. CALIBRATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "69G6mUZQvWIl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69G6mUZQvWIl",
        "outputId": "570a0ae1-3fc8-4574-d955-ff6dabb0fb3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "SECTION 9: CALIBRATION\n",
            "================================================================================\n",
            "Calibrating: ['Gradient Boosting (Optimized)', 'CatBoost (Optimized)', 'LightGBM (Tuned)']\n",
            "\n",
            "Gradient Boosting (Optimized)...\n",
            "  Original: 0.8008, Calibrated: 0.7987, Gain: -0.0021\n",
            "\n",
            "CatBoost (Optimized)...\n",
            "  Original: 0.8121, Calibrated: 0.8043, Gain: -0.0078\n",
            "\n",
            "LightGBM (Tuned)...\n",
            "  Original: 0.7872, Calibrated: 0.8067, Gain: +0.0195\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\nSECTION 9: CALIBRATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# --- Select Top 3 Models by CV Score ---\n",
        "top3 = top3 = df_res.nlargest(3, 'CV')['Model'].tolist()\n",
        "print(f\"Calibrating: {top3}\")\n",
        "\n",
        "# --- Perform Calibration using Isotonic Regression ---\n",
        "calib = {}\n",
        "\n",
        "for name in top3:\n",
        "    print(f\"\\n{name}...\")\n",
        "\n",
        "    r = results[name]\n",
        "    X_tr = X_train_bal_sc if (r['scaled'] and IMBLEARN_AVAILABLE) else X_train_sc if r['scaled'] else X_train_bal if IMBLEARN_AVAILABLE else X_train\n",
        "    y_tr = y_train_bal if IMBLEARN_AVAILABLE else y_train\n",
        "    X_v = X_val_sc if r['scaled'] else X_val\n",
        "\n",
        "    # Initialize and fit calibrated model\n",
        "    clf = CalibratedClassifierCV(r['model'], method='isotonic', cv=3)\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    \n",
        "    # Predict probabilities and evaluate ROC-AUC\n",
        "    pred = clf.predict_proba(X_v)[:, 1]\n",
        "    score = roc_auc_score(y_val, pred)\n",
        "\n",
        "    print(f\"  Original: {r['val']:.4f}, Calibrated: {score:.4f}, Gain: {score - r['val']:+.4f}\")\n",
        "\n",
        "    calib[f\"{name} (Cal)\"] = {'model': clf, 'val': score, 'pred': pred, 'scaled': r['scaled']}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ZG6scExidw",
      "metadata": {
        "id": "f5ZG6scExidw"
      },
      "source": [
        "## **🪢10. ENSEMBEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "58x5W8_Jx3s7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58x5W8_Jx3s7",
        "outputId": "f9df9cad-40c5-4117-9b15-94794d09e921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SECTION 10: ENSEMBLE (Auto, Calibrated and Safe)\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "==== MODEL PERFORMANCE SUMMARY (from Section 7) ====\n",
            "Logistic L2 (Tuned)                 | Val: 0.8321 | Gap: +0.0175\n",
            "Logistic L1 (Tuned)                 | Val: 0.7275 | Gap: -0.0354\n",
            "Random Forest (Optimized)           | Val: 0.7919 | Gap: +0.1269\n",
            "Extra Trees (Optimized)             | Val: 0.8215 | Gap: +0.1256\n",
            "Gradient Boosting (Optimized)       | Val: 0.8008 | Gap: +0.1898\n",
            "XGBoost (Optimized)                 | Val: 0.8091 | Gap: +0.0871\n",
            "LightGBM (Tuned)                    | Val: 0.7872 | Gap: +0.2086\n",
            "CatBoost (Optimized)                | Val: 0.8121 | Gap: +0.1550\n",
            "AdaBoost (Tuned)                    | Val: 0.7834 | Gap: +0.0729\n",
            "HistGradientBoosting (Fast)         | Val: 0.7648 | Gap: +0.2352\n",
            "Bagging (Logistic Base)             | Val: 0.8345 | Gap: +0.0020\n",
            "\n",
            "✅ Auto-selected models for ensemble: ['Logistic L2 (Tuned)', 'XGBoost (Optimized)', 'Bagging (Logistic Base)']\n",
            "\n",
            "🎯 Best weights: {'Logistic L2 (Tuned)': np.float64(0.2), 'XGBoost (Optimized)': np.float64(0.2), 'Bagging (Logistic Base)': np.float64(0.6)}\n",
            "Best blended AUC (val): 0.8325\n",
            "\n",
            "---- FINAL ENSEMBLE EVALUATION ----\n",
            "Final Ensemble Models : ['Logistic L2 (Tuned)', 'XGBoost (Optimized)', 'Bagging (Logistic Base)']\n",
            "Final Weights : [0.2 0.2 0.6]\n",
            "Final Ensemble AUC (val): 0.8325\n",
            "Gain vs Best Model: +0.0000\n",
            "\n",
            "----- COMPARISON: Individual vs Ensemble AUC ----\n",
            "Logistic L2 (Tuned)                : 0.8321\n",
            "XGBoost (Optimized)                : 0.8091\n",
            "Bagging (Logistic Base)            : 0.8345\n",
            "Final Ensemble                     : 0.8325\n",
            "\n",
            "💾 Ensemble results saved to 'ensemble_results.csv\n",
            "\n",
            "✅ Ensemble completed successfully and safely without errors\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSECTION 10: ENSEMBLE (Auto, Calibrated and Safe)\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# 0. Fallback and Setup\n",
        "if \"calib\" not in locals():\n",
        "    calib = {}\n",
        "    print(\"⚠️ 'calib' dictionary not found — skipping calibration, using original models\")\n",
        "\n",
        "# 1. Model Permormance Summary\n",
        "print(\"\\n==== MODEL PERFORMANCE SUMMARY (from Section 7) ====\")\n",
        "\n",
        "summary_rows = []\n",
        "for name, res in results.items():\n",
        "    summary_rows.append([name, res['val'], res['gap']])\n",
        "    print(f\"{name:35s} | Val: {res['val']:.4f} | Gap: {res['gap']:+.4f}\")\n",
        "\n",
        "#2. Auto Selection for Ensemble Candidates\n",
        "candidates = [\n",
        "    name for name, res in results.items()\n",
        "    if res['val'] >= 0.80 and abs(res['gap']) <= 0.10\n",
        "]\n",
        "print(\"\\n✅ Auto-selected models for ensemble:\", candidates)\n",
        "\n",
        "# Validasi ketersediaan model di results/calib\n",
        "candidates = [m for m in candidates if m in results or m in calib]\n",
        "assert len(candidates) >= 2, f\"❌ Not enough model candidates: {candidates}\"\n",
        "\n",
        "# Warn if all models are same type\n",
        "if len(set([\"Logistic\" in m for m in candidates])) == 1:\n",
        "    print(\"⚠️ All selected models are linear — consider adding a tree-based model for diversity\")\n",
        "\n",
        "#3. Prepare Validation Predictions (Post-Calibration)\n",
        "pred_mat = np.column_stack([\n",
        "    calib[m]['pred'] if m in calib else results[m]['pred']\n",
        "    for m in candidates\n",
        "])\n",
        "\n",
        "#4. Grid Seacrh for Optimal Ensemble Werights\n",
        "grid = [0.2, 0.4, 0.6, 0.8]\n",
        "best_auc, best_w = -1, None\n",
        "\n",
        "for ws in product(grid, repeat=len(candidates)):\n",
        "    if abs(sum(ws) - 1) > 1e-6:\n",
        "        continue\n",
        "    w = np.array(ws, dtype=float)\n",
        "    w /= np.sum(w)  \n",
        "    blend = pred_mat @ w\n",
        "    auc = roc_auc_score(y_val, blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "\n",
        "print(f\"\\n🎯 Best weights: {dict(zip(candidates, best_w))}\")\n",
        "print(f\"Best blended AUC (val): {best_auc:.4f}\")\n",
        "\n",
        "# Save best ensemble\n",
        "ens_models = candidates\n",
        "wts = best_w\n",
        "best_score = best_auc\n",
        "best_pred = pred_mat @ wts\n",
        "\n",
        "#5. Final Ensemble Evaluation\n",
        "print(\"\\n---- FINAL ENSEMBLE EVALUATION ----\")\n",
        "print(f\"Final Ensemble Models : {ens_models}\")\n",
        "print(f\"Final Weights : {np.round(wts, 3)}\")\n",
        "\n",
        "# Aggregate Predictions\n",
        "preds = []\n",
        "for m in ens_models:\n",
        "    if m in calib:\n",
        "        preds.append(calib[m]['pred'])\n",
        "    elif m in results:\n",
        "        preds.append(results[m]['pred'])\n",
        "    else:\n",
        "        print(f\"⚠️ Warning: {m} not found in results/calib\")\n",
        "\n",
        "pred_mat = np.column_stack(preds)\n",
        "final_pred = pred_mat @ wts\n",
        "\n",
        "# Evaluate ensemble performance\n",
        "final_auc = roc_auc_score(y_val, final_pred)\n",
        "gain_vs_best = final_auc - best_score\n",
        "\n",
        "print(f\"Final Ensemble AUC (val): {final_auc:.4f}\")\n",
        "print(f\"Gain vs Best Model: {gain_vs_best:+.4f}\")\n",
        "\n",
        "#6. Individual vs Ensemble Comparison\n",
        "print(\"\\n----- COMPARISON: Individual vs Ensemble AUC ----\")\n",
        "\n",
        "compare_rows = []\n",
        "for m in ens_models:\n",
        "    auc_val = roc_auc_score(y_val, results[m]['pred'])\n",
        "    compare_rows.append([m, auc_val])\n",
        "    print(f\"{m:35s}: {auc_val:.4f}\")\n",
        "\n",
        "print(f\"{'Final Ensemble':35s}: {final_auc:.4f}\")\n",
        "\n",
        "#Save Ensemble Results to CSV\n",
        "df_ens = pd.DataFrame({\n",
        "    'Model': [m for m, _ in compare_rows] + ['Final Ensemble'],\n",
        "    'AUC_Validation': [auc for _, auc in compare_rows] + [final_auc],\n",
        "    'Weight': list(best_w) + [None]\n",
        "})\n",
        "df_ens.to_csv(\"ensemble_results.csv\", index=False)\n",
        "print(\"\\n💾 Ensemble results saved to 'ensemble_results.csv\")\n",
        "\n",
        "#8. Visualize ROC Curve\n",
        "fig, ax = plt.subplots(figsize=(10, 7))\n",
        "\n",
        "for m in ens_models:\n",
        "    pred = calib[m]['pred'] if m in calib else results[m]['pred']\n",
        "    fpr, tpr, _ = roc_curve(y_val, pred)\n",
        "    auc_val = roc_auc_score(y_val, pred)\n",
        "    ax.plot(fpr, tpr, lw=1.8, label=f\"{m} ({auc_val:.3f})\")\n",
        "\n",
        "# Plot Ensemble Curve\n",
        "fpr, tpr, _ = roc_curve(y_val, final_pred)\n",
        "ax.plot(fpr, tpr, lw=3, linestyle=\"--\", color=\"hotpink\", label=f\"Ensemble ({final_auc:.3f})\")\n",
        "\n",
        "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curves – Final Ensemble (Post-Calibration)\", fontweight=\"bold\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"roc_ensemble_postcalib.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n✅ Ensemble completed successfully and safely without errors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yqeZ7ykExvd_",
      "metadata": {
        "id": "yqeZ7ykExvd_"
      },
      "source": [
        "## **🎯11.TEST PREDICTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "2fikPzI9xsmM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fikPzI9xsmM",
        "outputId": "69cb8e97-0f89-4f7d-da72-8601036de5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "SECTION 11: TEST PREDICTIONS\n",
            "--------------------------------------------------------------------------------\n",
            "Logistic L2 (Tuned): mean=0.4210\n",
            "XGBoost (Optimized): mean=0.1570\n",
            "Bagging (Logistic Base): mean=0.1665\n",
            "\n",
            "Final mean: 0.2155 | Train mean: 0.1616 | Diff: 0.0539\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\nSECTION 11: TEST PREDICTIONS\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "test_preds = []\n",
        "\n",
        "# Loop through each model in the ensemble and generate predictions\n",
        "for m in ens_models:\n",
        "    if m in results:\n",
        "        # Retrieve model and scaling flag from 'results'\n",
        "        model, scaled = results[m]['model'], results[m]['scaled']\n",
        "    elif 'calib' in locals() and m in calib:\n",
        "        # Retrieve model and scaling flag from calibrated results\n",
        "        model, scaled = calib[m]['model'], calib[m]['scaled']\n",
        "    else:\n",
        "        # Skip models that are not found in results or calibration\n",
        "        print(f\"⚠️ {m} not found in results/calib — skipped\")\n",
        "        continue\n",
        "\n",
        "    # Apply scaling if required\n",
        "    X_t = scaler.transform(X_test_sel) if scaled else X_test_sel\n",
        "\n",
        "    # Generate probability predictions for the positive class\n",
        "    p = model.predict_proba(X_t)[:, 1]\n",
        "    test_preds.append(p)\n",
        "\n",
        "    # Log the mean prediction value for sanity check\n",
        "    print(f\"{m}: mean={p.mean():.4f}\")\n",
        "\n",
        "# Ensure weight vector matches the number of valid predictions\n",
        "valid_wts = wts[:len(test_preds)]\n",
        "\n",
        "# Compute final weighted average of ensemble predictions\n",
        "final = np.average(np.column_stack(test_preds), axis=1, weights=valid_wts)\n",
        "\n",
        "# Display final summary\n",
        "print(\n",
        "    f\"\\nFinal mean: {final.mean():.4f} | \"\n",
        "    f\"Train mean: {y.mean():.4f} | \"\n",
        "    f\"Diff: {abs(final.mean() - y.mean()):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZmAoN3ENyD1t",
      "metadata": {
        "id": "ZmAoN3ENyD1t"
      },
      "source": [
        "## **✅ 12. SUBMISSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "bGTGXhLkyBEe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGTGXhLkyBEe",
        "outputId": "3051a330-7fdc-4c04-ccf0-1769036e0200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "SECTION 12: SUBMISSION\n",
            "--------------------------------------------------------------------------------\n",
            "Created: sample_submission.csv\n",
            "\n",
            "Mean: 0.2155, Std: 0.1360\n",
            "Min: 0.0362, Max: 0.7110\n",
            "\n",
            "📊 Saved visualization: submission.png\n",
            "\n",
            "Preview of submission file:\n",
            "\n",
            "id     Attrition\n",
            "CM617 0.3664    \n",
            "PJ010 0.1021    \n",
            "GJ831 0.1582    \n",
            "JD352 0.0593    \n",
            "WZ263 0.4655    \n",
            "OD346 0.2737    \n",
            "GF698 0.1254    \n",
            "JK198 0.1092    \n",
            "SP276 0.0682    \n",
            "OP259 0.5508    \n",
            "CR707 0.2169    \n",
            "HO168 0.1615    \n",
            "CX146 0.1679    \n",
            "NV992 0.2035    \n",
            "FB734 0.2306    \n",
            "XV265 0.5280    \n",
            "IK137 0.1076    \n",
            "NQ433 0.0718    \n",
            "EJ758 0.3803    \n",
            "LZ264 0.1416    \n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DONE! Submission successfully generated\n",
            "================================================================================\n",
            "CV: 0.8238, Val: 0.8345, Ensemble: 0.8325\n",
            "Files generated: sample_submission.csv + 4 PNG visualizations\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\nSECTION 12: SUBMISSION\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Create Submission File\n",
        "sub = pd.DataFrame({'id': test_ids, 'Attrition': final}) \n",
        "\n",
        "sub.to_csv('sample_submission.csv', index=False)\n",
        "print(\"Created: sample_submission.csv\")\n",
        "\n",
        "# Basic Statistic of Submission Values\n",
        "print(f\"\\nMean: {sub['Attrition'].mean():.4f}, Std: {sub['Attrition'].std():.4f}\")\n",
        "print(f\"Min: {sub['Attrition'].min():.4f}, Max: {sub['Attrition'].max():.4f}\")\n",
        "\n",
        "# Visualize distribution & boxplot of submission predictions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# --- Histogram ---\n",
        "axes[0].hist(sub['Attrition'], bins=50, color='pink', alpha=0.7, edgecolor='black')\n",
        "axes[0].axvline(x=sub['Attrition'].mean(), color='red', linestyle='--', label='Test')\n",
        "axes[0].axvline(x=y.mean(), color='blue', linestyle='--', label='Train')\n",
        "axes[0].set_title('Distribution', fontweight='bold')\n",
        "axes[0].legend()\n",
        "\n",
        "# --- Boxplot ---\n",
        "axes[1].boxplot([sub['Attrition']])\n",
        "axes[1].axhline(y=y.mean(), color='blue', linestyle='--')\n",
        "axes[1].set_title('Boxplot', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('submission.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n📊 Saved visualization: submission.png\")\n",
        "\n",
        "# Preview first few submission rows\n",
        "print(\"\\nPreview of submission file:\")\n",
        "print(\"\\n\" + sub.head(20).to_string(index=False))\n",
        "\n",
        "# Final Summary\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"DONE! Submission successfully generated\")\n",
        "print(\"=\"*80)\n",
        "print(f\"CV: {df_res['CV'].max():.4f}, Val: {df_res['Val'].max():.4f}, Ensemble: {best_score:.4f}\")\n",
        "print(\"Files generated: sample_submission.csv + 4 PNG visualizations\")\n",
        "print(\"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869d2679",
      "metadata": {},
      "source": [
        "## **🗃️ 13. SAVE BEST MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "43a4ca83",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "SECTION 13: SAVE BEST MODEL\n",
            "--------------------------------------------------------------------------------\n",
            "🏆 Best Model: Bagging (Logistic Base)\n",
            "Validation AUC: 0.8345, Gap: +0.0020\n",
            "✅ Model saved successfully as models/Bagging_Logistic_Base__v1.pkl\n",
            "📦 Bundle (model + scaler + metadata) saved as models/best_model_bundle_v1.pkl\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\nSECTION 13: SAVE BEST MODEL\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Ensure folder exists\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Find best model automatically based on validation AUC\n",
        "best_name = max(results, key=lambda x: results[x][\"val\"])\n",
        "best_model = results[best_name][\"model\"]\n",
        "\n",
        "print(f\"🏆 Best Model: {best_name}\")\n",
        "print(f\"Validation AUC: {results[best_name]['val']:.4f}, Gap: {results[best_name]['gap']:+.4f}\")\n",
        "\n",
        "# Save model to .pkl\n",
        "filename = f\"models/{best_name.replace(' ', '_').replace('(', '').replace(')', '')}__v1.pkl\"\n",
        "with open(filename, \"wb\") as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "print(f\"✅ Model saved successfully as {filename}\")\n",
        "\n",
        "# (Optional) Save bundle with scaler + metadata\n",
        "bundle = {\n",
        "    \"model_name\": best_name,\n",
        "    \"model\": best_model,\n",
        "    \"scaler\": scaler if 'scaler' in locals() else None,\n",
        "    \"features\": list(X_train.columns),\n",
        "    \"val_score\": results[best_name][\"val\"],\n",
        "    \"gap\": results[best_name][\"gap\"]\n",
        "}\n",
        "\n",
        "with open(\"models/best_model_bundle_v1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(bundle, f)\n",
        "\n",
        "print(\"📦 Bundle (model + scaler + metadata) saved as models/best_model_bundle_v1.pkl\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
